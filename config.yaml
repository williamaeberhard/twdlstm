# Configuration for twdlstm v0.4.4

path_data: '/mydata/forestcast/william/WP3/DataProcessed'
path_outputdir: '/mydata/forestcast/william/WP3/LSTM_runs/outputs'
path_checkpointdir: '/mydata/forestcast/william/WP3/LSTM_runs/checkpoints'

prefixoutput: '00' #

tstoy: '05' # '04' for tstoy04, '05' for tstoy05

# series: ["10", "28", "35", "36", "37", "38", "39"] # full
# ^ Picea abies prop samp at 3 selected sites (among 4 after excl 1 series)
# series_te: ["11", "12", "16", "17", "29", "40", "41"] # tstoy04 all picea abies
# series_trva: ["10", "28"] # tstoy04 picea abies
# series_te: ["11", "12", "16"] # tstoy04 picea abies
series_trva: ["05", "07", "08"] # tstoy05 Picea abies
series_te: ["15", "19"] # tstoy05 Picea abies
# ^ trva (tr and va) used by train.py, te only used by test.py

# date_t0: '2021-01-01' # assuming time points are daily
# ind_t0: 366 # 2021-01-01
date_t0: '2020-01-01' # assuming time points are daily
# nT_tr: 366 # size of tr set (time points), starts from date_t0
# nT_va: 100 # size of va set, starts right after the nT_tr time points
nT: 20 # subset size (time points), split in batches, starts from date_t0
prop_va: 0.2 # proportion of data batches held-out for va set
srs_seed: 1234 # seed for simple random sampling for va subset

batch_len: 5 # length of batches, both for tr and va
# loss_hor: 1 # nb values at end of each batch contribute to tr/va loss
# ^ v0.4.2: deprecated, hard-coded to 1, i.e. only last obs of batch contributes

covvec: ['pr', 'at']
# covvec: ['pr', 'at', 'ws', 'dp', 'sr', 'lr'] # all in tstoy04

h_size: 32 # 8,16,32,64 <= 32 generally good
o_size: 1 # 1
nb_layers: 1 # 1

lambda_LaplacianReg: 0.01 # 0.0, 0.01, 0.05
# ^ multiplies sum abs diff pred within each batch
len_reg: 10 # min(10, batch_len/4)
# ^ nb pred that are regularized in each batch, must be < batch_len

torch_seed: 123 # torch.randn

maxepoch: 20 # 50, 100, 200, 500
step_ckpt: 10 # 10
# ^ print and record tr/va loss every maxepoch/step_ckpt epoch

loss: 'MAE' # 'MSE', 'MAE'
optim: 'RMSprop' # 'RMSprop', 'Adam', 'AdamW', 'RAdam'
learning_rate: 1e-2 # 1e-2 is good, 1e-1 better if scheduling
alphal2: 0.1 # L2 pen (weight decay) # 0.0, 0.5, 0.8
momentum: 0.0 # momentum in RMSprop # 0.0, 0.5

# run from /mydata/forestcast/william/WP3:
# python src/twdlstm/train.py LSTM_runs/configs/config_00.yaml > LSTM_runs/logs/log_00.txt
# python src/twdlstm/test.py LSTM_runs/configs/config_00.yaml > LSTM_runs/logs/log_te_00.txt
